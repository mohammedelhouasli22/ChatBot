{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('wordnet')\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "nltk.download('stopwords') \r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "from nltk.stem import wordnet # to perform lemmitization\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\r\n",
    "from nltk import pos_tag # for parts of speech\r\n",
    "from sklearn.metrics import pairwise_distances # to perfrom cosine similarity\r\n",
    "from nltk import word_tokenize # to create tokens\r\n",
    "from nltk.corpus import stopwords # for stop words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohammed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mohammed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mohammed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mohammed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df=pd.read_excel('dialog.xlsx') #pour lire la base de données des qustions réponses\r\n",
    "df.head(20) #pour voir les premiers 20 lignes de la base de données "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.ffill(axis = 0,inplace=True) #Pour remplir les valeurs nulles par la valeur précédente\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1=df.head(10) #pour copier les premiers 10 lignes de \"df\" vers \"df1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "# fonction pour convertir le texte vers miniscule et supprimer les caractères spéciaux\r\n",
    "\r\n",
    "def step1(x):\r\n",
    "    for i in x:\r\n",
    "        a=str(i).lower()\r\n",
    "        p=re.sub(r'[^a-z0-9]',' ',a)\r\n",
    "        print(p)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    " # word tokenizing\r\n",
    "    \r\n",
    "s='I want to buy a laptop'\r\n",
    "words=word_tokenize(s)\r\n",
    "print(words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lemma = wordnet.WordNetLemmatizer() # intialisation lemmatizer\r\n",
    "lemma.lemmatize('absorbed', pos = 'v')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_tag(nltk.word_tokenize(s),tagset = None) # returns the parts of speech of every word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function that performs text normalization steps\r\n",
    "\r\n",
    "def text_normalization(text):\r\n",
    "    text=str(text).lower() # text vers miniscule\r\n",
    "    spl_char_text=re.sub(r'[^ a-z]','',text) # supprimer les caractères speciaux\r\n",
    "    tokens=nltk.word_tokenize(spl_char_text) # word tokenizing\r\n",
    "    lema=wordnet.WordNetLemmatizer() # intialisation lemmatization\r\n",
    "    tags_list=pos_tag(tokens,tagset=None) # parts of speech\r\n",
    "    lema_words=[]   # list vide\r\n",
    "    for token,pos_token in tags_list:\r\n",
    "        if pos_token.startswith('V'):  # Verb\r\n",
    "            pos_val='v'\r\n",
    "        elif pos_token.startswith('J'): # Adjective\r\n",
    "            pos_val='a'\r\n",
    "        elif pos_token.startswith('R'): # Adverb\r\n",
    "            pos_val='r'\r\n",
    "        else:\r\n",
    "            pos_val='n' # Noun\r\n",
    "        lema_token=lema.lemmatize(token,pos_val) # performing lemmatization\r\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list\r\n",
    "    \r\n",
    "    return \" \".join(lema_words) # returns the lemmatized tokens as a sentence "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_normalization('I want to buy a computer')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "df['lemmatized_text']=df['Context'].apply(text_normalization) # applying the fuction to the dataset to get clean text\r\n",
    "df.tail(15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# all the stop words we have \r\n",
    "\r\n",
    "stop = stopwords.words('english')\r\n",
    "print(stop)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bag Of Words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "cv = CountVectorizer() # intializing the count vectorizer\r\n",
    "X = cv.fit_transform(df['lemmatized_text']).toarray()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# returns all the unique word from data \r\n",
    "\r\n",
    "features = cv.get_feature_names()\r\n",
    "df_bow = pd.DataFrame(X, columns = features)\r\n",
    "df_bow.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "Question ='I want a computer' # exemple"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "source": [
    "# checking for stop words\r\n",
    "\r\n",
    "Q=[]\r\n",
    "a=Question.split()\r\n",
    "for i in a:\r\n",
    "    if i in stop:\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        Q.append(i)\r\n",
    "    b=\" \".join(Q) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "Question_lemma = text_normalization(b) # applying the function that we created for text normalizing\r\n",
    "Question_bow = cv.transform([Question_lemma]).toarray() # applying bow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_normalization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Question_bow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# similarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cosine similarity for the above question we considered.\r\n",
    "\r\n",
    "cosine_value = 1- pairwise_distances(df_bow, Question_bow, metric = 'cosine' )\r\n",
    "(cosine_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "df['similarity_bow']=cosine_value # creating a new column "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_simi = pd.DataFrame(df, columns=['Text Response','similarity_bow']) # taking similarity value of responses for the question we took\r\n",
    "df_simi "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_simi_sort = df_simi.sort_values(by='similarity_bow', ascending=False) # sorting the values\r\n",
    "df_simi_sort.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "threshold = 0.2 # considering the value of p=smiliarity to be greater than 0.2\r\n",
    "df_threshold = df_simi_sort[df_simi_sort['similarity_bow'] > threshold] \r\n",
    "df_threshold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "index_value = cosine_value.argmax() # returns the index number of highest value\r\n",
    "index_value "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(Question)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Text Response'].loc[index_value] # The text at the above index becomes the response for the question"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function that removes stop words and process the text\r\n",
    "\r\n",
    "def stopword_(text):   \r\n",
    "    tag_list=pos_tag(nltk.word_tokenize(text),tagset=None)\r\n",
    "    stop=stopwords.words('english')\r\n",
    "    lema=wordnet.WordNetLemmatizer()\r\n",
    "    lema_word=[]\r\n",
    "    for token,pos_token in tag_list:\r\n",
    "        if token in stop:\r\n",
    "            continue\r\n",
    "        if pos_token.startswith('V'):\r\n",
    "            pos_val='v'\r\n",
    "        elif pos_token.startswith('J'):\r\n",
    "            pos_val='a'\r\n",
    "        elif pos_token.startswith('R'):\r\n",
    "            pos_val='r'\r\n",
    "        else:\r\n",
    "            pos_val='n'\r\n",
    "        lema_token=lema.lemmatize(token,pos_val)\r\n",
    "        lema_word.append(lema_token)\r\n",
    "    return \" \".join(lema_word) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# tf-idf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "Question1 ='i want a computer'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "# using tf-idf\r\n",
    "\r\n",
    "tfidf=TfidfVectorizer() # intializing tf-id \r\n",
    "x_tfidf=tfidf.fit_transform(df['lemmatized_text']).toarray() # transforming the data into array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "Question_lemma1 = text_normalization(Question1)\r\n",
    "Question_tfidf = tfidf.transform([Question_lemma1]).toarray() # applying tf-idf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# returns all the unique word from data with a score of that word\r\n",
    "\r\n",
    "df_tfidf=pd.DataFrame(x_tfidf,columns=tfidf.get_feature_names()) \r\n",
    "df_tfidf.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# similarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cos=1-pairwise_distances(df_tfidf,Question_tfidf,metric='cosine')  # applying cosine similarity\r\n",
    "cos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['similarity_tfidf']=cos # creating a new column \r\n",
    "df_simi_tfidf = pd.DataFrame(df, columns=['Text Response','similarity_tfidf']) # taking similarity value of responses for the question we took\r\n",
    "df_simi_tfidf "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_simi_tfidf_sort = df_simi_tfidf.sort_values(by='similarity_tfidf', ascending=False) # sorting the values\r\n",
    "df_simi_tfidf_sort.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "threshold = 0.2 # considering the value of p=smiliarity to be greater than 0.2\r\n",
    "df_threshold = df_simi_tfidf_sort[df_simi_tfidf_sort['similarity_tfidf'] > threshold] \r\n",
    "df_threshold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "index_value1 = cos.argmax() # returns the index number of highest value\r\n",
    "index_value1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Question1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Text Response'].loc[index_value1]  # returns the text at that index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Using Bag of Words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "# Function that removes stop words and process the text\r\n",
    "\r\n",
    "def stopword_(text):   \r\n",
    "    tag_list=pos_tag(nltk.word_tokenize(text),tagset=None)\r\n",
    "    stop=stopwords.words('english')\r\n",
    "    lema=wordnet.WordNetLemmatizer()\r\n",
    "    lema_word=[]\r\n",
    "    for token,pos_token in tag_list:\r\n",
    "        if token in stop:\r\n",
    "            continue\r\n",
    "        if pos_token.startswith('V'):\r\n",
    "            pos_val='v'\r\n",
    "        elif pos_token.startswith('J'):\r\n",
    "            pos_val='a'\r\n",
    "        elif pos_token.startswith('R'):\r\n",
    "            pos_val='r'\r\n",
    "        else:\r\n",
    "            pos_val='n'\r\n",
    "        lema_token=lema.lemmatize(token,pos_val)\r\n",
    "        lema_word.append(lema_token)\r\n",
    "    return \" \".join(lema_word) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "# defining a function that returns response to query using bow\r\n",
    "\r\n",
    "def chat_bow(text):\r\n",
    "    s=stopword_(text)\r\n",
    "    lemma=text_normalization(s) # calling the function to perform text normalization\r\n",
    "    bow=cv.transform([lemma]).toarray() # applying bow\r\n",
    "    cosine_value = 1- pairwise_distances(df_bow,bow, metric = 'cosine' )\r\n",
    "    index_value=cosine_value.argmax() # getting index value \r\n",
    "    return df['Text Response'].loc[index_value]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Using tf-idf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "# defining a function that returns response to query using tf-idf\r\n",
    "\r\n",
    "def chat_tfidf(text):\r\n",
    "    lemma=text_normalization(text) # calling the function to perform text normalization\r\n",
    "    tf=tfidf.transform([lemma]).toarray() # applying tf-idf\r\n",
    "    cos=1-pairwise_distances(df_tfidf,tf,metric='cosine') # applying cosine similarity\r\n",
    "    index_value=cos.argmax() # getting index value \r\n",
    "    return df['Text Response'].loc[index_value]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}